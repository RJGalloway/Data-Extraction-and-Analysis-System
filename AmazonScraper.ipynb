{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf1f421-a16e-4299-b3fc-466e317c39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ryan Galloway\n",
    "#CS5396\n",
    "#April 2nd, 2021\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import mysql.connector\n",
    "\n",
    "noPages = 2\n",
    "\n",
    "def getData(pageNo): \n",
    "    #setting user agaent header to try and fool Amazon so it doesn't know we are scraping (not working) when using request\n",
    "    #https://www.codementor.io/blog/python-web-scraping-63l2v9sf2q   ==> Header Inspection\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    \n",
    "    #Creates request object from the given url and parameters\n",
    "    url = requests.get('https://www.amazon.com/gp/bestsellers/books/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo), headers=headers)\n",
    "    \n",
    "    #set content varaible = to content of the url request object (html code scraped)\n",
    "    content = url.content\n",
    "    \n",
    "    #create a BeautifulSoup object using the content (soupify it)\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup.prettify)\n",
    "    \n",
    "    #Holds the data we are scraping for all pages\n",
    "    scrapedData = []\n",
    "    \n",
    "    #Parses the HTML content from the \"parent tag\" (tag that holds all the data about the books)\n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
    "        \n",
    "        #each of these variables holds the attribute value of the item we are looking for\n",
    "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "        n = name.find_all('img', alt=True)\n",
    "        author = d.find('a', attrs={'class':'a-size-small a-link-child'})\n",
    "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
    "        usersRated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
    "        price = d.find('span', attrs={'class':'p13n-sc-price'})\n",
    "\n",
    "        #temp list to hold data scraped for each iteration of scraping\n",
    "        temp=[]\n",
    "        \n",
    "        #These are checking the objects returned from the .find BS function call\n",
    "        #which is why is not None is used, to test if the variable \n",
    "        #is valid. If an object has no value associated\n",
    "        #set a default value.\n",
    "        if name is not None:\n",
    "            temp.append(n[0]['alt'])\n",
    "        else:\n",
    "            temp.append(\"unknown-product\")\n",
    "            \n",
    "        if author is not None:\n",
    "            #print(author.text)\n",
    "            temp.append(author.text)\n",
    "        elif author is None:\n",
    "            author = d.find('span', attrs={'class':'a-size-small a-color-base'})\n",
    "            if author is not None:\n",
    "                temp.append(author.text)\n",
    "            else:    \n",
    "                temp.append('0')\n",
    "                \n",
    "        if rating is not None:\n",
    "            #print(rating.text)\n",
    "            temp.append(rating.text)\n",
    "        else:\n",
    "            temp.append('-1')\n",
    "\n",
    "        if usersRated is not None:\n",
    "            #print(price.text)\n",
    "            temp.append(usersRated.text)\n",
    "        else:\n",
    "            temp.append('0')     \n",
    "\n",
    "        if price is not None:\n",
    "            #print(price.text)\n",
    "            temp.append(price.text)\n",
    "        else:\n",
    "            temp.append('0')\n",
    "        \n",
    "        #master list of scraped data\n",
    "        scrapedData.append(temp)    \n",
    "    return scrapedData\n",
    "\n",
    "#List to hold returned data scraped\n",
    "results = []\n",
    "for i in range(1, noPages+1):\n",
    "    results.append(getData(i))\n",
    "#print (results[0], 'alt')\n",
    "    \n",
    "#initialize flatten: This variable will flatten the results of separate sublists\n",
    "#when we create the data frame using the results list\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "#create the data frame\n",
    "dataFrame = pd.DataFrame(flatten(results),columns=['Book Name','Author','Rating','Customers_Rated', 'Price'])\n",
    "#write data frame to csv file\n",
    "dataFrame.to_csv('amazonBooks.csv', index=False, encoding='utf-8')\n",
    "\n",
    "#read the csv for processing\n",
    "dataFrame = pd.read_csv(\"amazonBooks.csv\")\n",
    "\n",
    "\n",
    "#Remove out of 5 from ratings column, lamba expression splits the string after the \n",
    "#rating\n",
    "dataFrame['Rating'] = dataFrame['Rating'].apply(lambda x: x.split()[0])\n",
    "\n",
    "#convert rating to numeric from string \n",
    "dataFrame['Rating'] = pd.to_numeric(dataFrame['Rating'])\n",
    "\n",
    "#remove $ from price\n",
    "dataFrame[\"Price\"] = dataFrame[\"Price\"].str.replace('$', ' ', regex = True)\n",
    "\n",
    "\n",
    "#turn price into float\n",
    "dataFrame[\"Price\"] = dataFrame[\"Price\"].astype(float)\n",
    "\n",
    "#Remove comma from number of ratings\n",
    "dataFrame[\"Customers_Rated\"] = dataFrame[\"Customers_Rated\"].str.replace(',', '', regex = True)\n",
    "\n",
    "#convert rating string to numeric, \n",
    "dataFrame[\"Customers_Rated\"] = pd.to_numeric(dataFrame[\"Customers_Rated\"], errors = 'ignore')\n",
    "\n",
    "dataFrame.dtypes\n",
    "\n",
    "dataFrame.to_csv('AmznBooksTransformed.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facb318-99fa-40a7-b93a-a38b44cb2458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9c550-9b1f-43f2-8a09-cf5d02aa74c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bb961-f1fd-430e-b27e-f900116268d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
